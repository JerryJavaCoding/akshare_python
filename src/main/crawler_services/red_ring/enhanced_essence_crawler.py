#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå°çº¢åœˆç²¾åå¸–å­çˆ¬è™« - æ”¯æŒç™»å½•çŠ¶æ€ä¿æŒå’Œåˆ†é¡µæŠ“å–
"""

import asyncio
import sqlite3
import time
import random
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EnhancedEssenceCrawler:
    def __init__(self, db_path: str = "red_ring_essence.db"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆç²¾åå¸–å­çˆ¬è™«
        
        Args:
            db_path: SQLiteæ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = db_path
        self.base_url = "https://www.red-ring.cn/group/14775"
        self.essence_url = f"{self.base_url}?tab=essence"
        self.login_state_file = "login_state.json"
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºç²¾åå¸–å­è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS essence_posts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT,
                author TEXT,
                publish_date TEXT,
                likes_count INTEGER DEFAULT 0,
                comments_count INTEGER DEFAULT 0,
                view_count INTEGER DEFAULT 0,
                is_pinned BOOLEAN DEFAULT FALSE,
                tags TEXT,
                summary TEXT,
                page_number INTEGER DEFAULT 1,
                post_url TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºè¯„è®ºå›å¤è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS post_comments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                post_id INTEGER,
                comment_author TEXT,
                comment_content TEXT,
                comment_date TEXT,
                is_author_reply BOOLEAN DEFAULT FALSE,
                reply_to_comment_id INTEGER,
                likes_count INTEGER DEFAULT 0,
                comment_type TEXT DEFAULT 'comment',  -- comment, reply
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (post_id) REFERENCES essence_posts (id)
            )
        ''')
        
        # åˆ›å»ºæŠ“å–è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS crawl_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                crawl_date TEXT,
                pages_crawled INTEGER,
                posts_count INTEGER,
                comments_count INTEGER DEFAULT 0,
                success BOOLEAN,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç™»å½•çŠ¶æ€è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS login_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"å¢å¼ºç‰ˆç²¾åå¸–å­æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    async def ensure_login(self, page):
        """
        ç¡®ä¿ç”¨æˆ·å·²ç™»å½•ï¼Œå¦‚æœæœªç™»å½•åˆ™æç¤ºç”¨æˆ·æ‰«ç ç™»å½•
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            
        Returns:
            æ˜¯å¦å·²ç™»å½•
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
        is_logged_in = await self._check_login_status(page)
        
        if is_logged_in:
            logger.info("ç”¨æˆ·å·²ç™»å½•")
            return True
        
        # å¦‚æœæœªç™»å½•ï¼Œæç¤ºç”¨æˆ·æ‰«ç ç™»å½•
        logger.info("æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€ï¼Œè¯·æ‰«ç ç™»å½•...")
        print("\nğŸ” è¯·æ‰«ç ç™»å½•å°çº¢åœˆè´¦å·")
        print("   ç³»ç»Ÿå°†æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œè¯·ä½¿ç”¨å¾®ä¿¡æ‰«ç ç™»å½•")
        print("   ç™»å½•æˆåŠŸåï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç»§ç»­æŠ“å–æ•°æ®")
        print("   " + "-" * 40)
        
        # å¯¼èˆªåˆ°ç™»å½•é¡µé¢
        await page.goto("https://www.red-ring.cn/login", wait_until='networkidle')
        
        # ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•
        login_success = await self._wait_for_login(page)
        
        if login_success:
            logger.info("ç”¨æˆ·ç™»å½•æˆåŠŸ")
            # ä¿å­˜ç™»å½•çŠ¶æ€
            await self._save_login_state(page)
            return True
        else:
            logger.warning("ç”¨æˆ·ç™»å½•è¶…æ—¶æˆ–å¤±è´¥")
            return False
    
    async def _check_login_status(self, page) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç™»å½•"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•ç›¸å…³çš„å…ƒç´ 
            login_elements = await page.query_selector_all('.login-btn, .login-button, [href*="login"]')
            user_elements = await page.query_selector_all('.user-avatar, .user-info, .user-name')
            
            # å¦‚æœæœ‰ç”¨æˆ·ä¿¡æ¯å…ƒç´ ä¸”æ²¡æœ‰ç™»å½•æŒ‰é’®ï¼Œåˆ™è®¤ä¸ºå·²ç™»å½•
            if len(user_elements) > 0 and len(login_elements) == 0:
                return True
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜æˆ–å†…å®¹ä¸­æ˜¯å¦åŒ…å«ç™»å½•ç›¸å…³ä¿¡æ¯
            page_content = await page.content()
            if "ç™»å½•" in page_content and "æ³¨å†Œ" in page_content:
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def _wait_for_login(self, page, timeout: int = 120) -> bool:
        """ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
            is_logged_in = await self._check_login_status(page)
            
            if is_logged_in:
                return True
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
            await asyncio.sleep(5)
            print("â³ ç­‰å¾…ç™»å½•ä¸­... (è¯·ä½¿ç”¨å¾®ä¿¡æ‰«ç )")
        
        return False
    
    async def _save_login_state(self, page):
        """ä¿å­˜ç™»å½•çŠ¶æ€"""
        try:
            # è·å–cookies
            cookies = await page.context.cookies()
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            state_data = {
                'cookies': cookies,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(self.login_state_file, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, ensure_ascii=False, indent=2)
            
            logger.info("ç™»å½•çŠ¶æ€å·²ä¿å­˜")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
    
    async def _load_login_state(self, context):
        """åŠ è½½ç™»å½•çŠ¶æ€"""
        try:
            if not os.path.exists(self.login_state_file):
                return False
            
            with open(self.login_state_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            # æ£€æŸ¥çŠ¶æ€æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
            saved_at = datetime.fromisoformat(state_data['saved_at'])
            if (datetime.now() - saved_at).total_seconds() > 24 * 3600:
                logger.info("ç™»å½•çŠ¶æ€å·²è¿‡æœŸ")
                return False
            
            # è®¾ç½®cookies
            await context.add_cookies(state_data['cookies'])
            logger.info("ç™»å½•çŠ¶æ€å·²åŠ è½½")
            return True
            
        except Exception as e:
            logger.warning(f"åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def crawl_essence_posts_with_comments(self, max_pages: int = 10):
        """
        ä½¿ç”¨åˆ†é¡µæŠ“å–ç²¾åå¸–å­åŠå…¶è¯„è®ºå›å¤
        
        Args:
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°
            
        Returns:
            æŠ“å–æ˜¯å¦æˆåŠŸ
        """
        try:
            from playwright.async_api import async_playwright
            
            logger.info(f"å¼€å§‹åˆ†é¡µæŠ“å–ç²¾åå¸–å­åŠè¯„è®ºï¼Œæœ€å¤š {max_pages} é¡µ")
            
            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨
                browser = await p.chromium.launch(
                    headless=False,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-features=VizDisplayCompositor',
                        '--disable-background-timer-throttling',
                        '--disable-backgrounding-occluded-windows',
                        '--disable-renderer-backgrounding'
                    ]
                )
                
                # åˆ›å»ºä¸Šä¸‹æ–‡
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    extra_http_headers={
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Accept-Encoding': 'gzip, deflate, br',
                    }
                )
                
                # å°è¯•åŠ è½½ç™»å½•çŠ¶æ€
                await self._load_login_state(context)
                
                page = await context.new_page()
                
                # å¯¼èˆªåˆ°ç²¾åé¡µé¢
                logger.info(f"å¯¼èˆªåˆ°ç²¾åé¡µé¢: {self.essence_url}")
                await page.goto(self.essence_url, wait_until='networkidle')
                
                # ç¡®ä¿ç™»å½•
                if not await self.ensure_login(page):
                    logger.error("ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æŠ“å–")
                    await browser.close()
                    return False
                
                # åˆ†é¡µæŠ“å–
                all_posts = []
                total_comments = 0
                current_page = 1
                
                while current_page <= max_pages:
                    logger.info(f"å¼€å§‹æŠ“å–ç¬¬ {current_page} é¡µ")
                    
                    try:
                        # ç­‰å¾…é¡µé¢ç¨³å®š
                        await page.wait_for_load_state('networkidle')
                        await page.wait_for_timeout(2000)
                        
                        # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                        await self._scroll_page(page)
                        
                        # è·å–é¡µé¢å†…å®¹
                        content = await self._get_page_content(page)
                        
                        if not content:
                            logger.warning(f"ç¬¬ {current_page} é¡µæœªèƒ½è·å–åˆ°å†…å®¹")
                            break
                        
                        # è§£æç²¾åå¸–å­
                        page_posts = self._parse_essence_posts(content)
                        logger.info(f"ç¬¬ {current_page} é¡µè§£æå‡º {len(page_posts)} ä¸ªç²¾åå¸–å­")
                        
                        # æ ‡è®°é¡µç 
                        for post in page_posts:
                            post['page_number'] = current_page
                        
                        # æŠ“å–æ¯ä¸ªå¸–å­çš„è¯„è®ºå’Œå›å¤
                        for post in page_posts:
                            logger.info(f"å¼€å§‹æŠ“å–å¸–å­è¯„è®º: {post.get('title', '')[:50]}...")
                            comments = await self._get_post_comments(page, post)
                            post['comments'] = comments
                            total_comments += len(comments)
                            logger.info(f"å¸–å­ '{post.get('title', '')[:30]}...' æŠ“å–åˆ° {len(comments)} æ¡è¯„è®º")
                        
                        all_posts.extend(page_posts)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                        has_next_page = await self._go_to_next_page(page)
                        if not has_next_page:
                            logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                            break
                        
                        current_page += 1
                        await self._random_delay(2, 4)  # é¡µé¢é—´å»¶è¿Ÿ
                        
                    except Exception as e:
                        logger.error(f"ç¬¬ {current_page} é¡µæŠ“å–å¤±è´¥: {e}")
                        break
                
                logger.info(f"æ€»å…±æŠ“å– {len(all_posts)} ä¸ªç²¾åå¸–å­ï¼Œ{total_comments} æ¡è¯„è®ºï¼Œæ¥è‡ª {current_page} é¡µ")
                
                # è¿‡æ»¤è¿‘åŠå¹´çš„å¸–å­
                recent_posts = self._filter_recent_half_year(all_posts)
                logger.info(f"è¿‘åŠå¹´çš„ç²¾åå¸–å­: {len(recent_posts)} ä¸ª")
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if recent_posts:
                    saved_posts, saved_comments = self._save_posts_with_comments(recent_posts)
                    self._log_crawl_success_with_comments(current_page, saved_posts, saved_comments)
                else:
                    logger.warning("æ²¡æœ‰æ‰¾åˆ°è¿‘åŠå¹´çš„ç²¾åå¸–å­")
                    self._log_crawl_failure("æ²¡æœ‰æ‰¾åˆ°è¿‘åŠå¹´çš„ç²¾åå¸–å­")
                
                # å…³é—­æµè§ˆå™¨
                await browser.close()
                
                return True
                
        except Exception as e:
            logger.error(f"åˆ†é¡µæŠ“å–ç²¾åå¸–å­åŠè¯„è®ºå¤±è´¥: {e}")
            self._log_crawl_failure(str(e))
            return False
    
    async def _get_post_comments(self, page, post: Dict) -> List[Dict]:
        """è·å–å¸–å­çš„è¯„è®ºå’Œå›å¤"""
        try:
            comments = []
            
            # æŸ¥æ‰¾å¸–å­å…ƒç´ å¹¶ç‚¹å‡»æŸ¥çœ‹è¯„è®º
            post_title = post.get('title', '')
            logger.info(f"å°è¯•æŸ¥æ‰¾å¸–å­: {post_title[:50]}...")
            
            # æŸ¥æ‰¾åŒ…å«å¸–å­æ ‡é¢˜çš„å…ƒç´ 
            post_elements = await page.query_selector_all(f'[class*="post"], [class*="article"], [class*="content"]')
            
            for element in post_elements:
                element_text = await element.text_content()
                if element_text and post_title[:30] in element_text:
                    logger.info(f"æ‰¾åˆ°å¸–å­å…ƒç´ ï¼Œå°è¯•ç‚¹å‡»æŸ¥çœ‹è¯„è®º")
                    
                    # æŸ¥æ‰¾è¯„è®ºæŒ‰é’®
                    comment_buttons = await element.query_selector_all('[class*="comment"], [class*="reply"], button:has-text("è¯„è®º"), button:has-text("æŸ¥çœ‹è¯„è®º")')
                    
                    for button in comment_buttons:
                        try:
                            await button.click()
                            await page.wait_for_timeout(3000)
                            logger.info("å·²ç‚¹å‡»è¯„è®ºæŒ‰é’®")
                            
                            # ç­‰å¾…è¯„è®ºåŒºåŸŸåŠ è½½
                            await page.wait_for_timeout(2000)
                            
                            # è·å–è¯„è®ºå†…å®¹
                            comments_content = await self._extract_comments_from_page(page)
                            comments.extend(comments_content)
                            break
                            
                        except Exception as e:
                            logger.warning(f"ç‚¹å‡»è¯„è®ºæŒ‰é’®å¤±è´¥: {e}")
                            continue
                    
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°è¯„è®ºæŒ‰é’®ï¼Œå°è¯•ä»å½“å‰é¡µé¢è§£æè¯„è®º
            if not comments:
                logger.info("æœªæ‰¾åˆ°è¯„è®ºæŒ‰é’®ï¼Œå°è¯•ä»é¡µé¢å†…å®¹è§£æè¯„è®º")
                comments = self._parse_comments_from_content(post.get('content', ''))
            
            return comments
            
        except Exception as e:
            logger.error(f"è·å–å¸–å­è¯„è®ºå¤±è´¥: {e}")
            return []
    
    async def _extract_comments_from_page(self, page) -> List[Dict]:
        """ä»é¡µé¢ä¸­æå–è¯„è®ºå†…å®¹"""
        try:
            comments = []
            
            # æ‰§è¡ŒJavaScriptæå–è¯„è®º
            comments_data = await page.evaluate("""
                () => {
                    const comments = [];
                    
                    // æŸ¥æ‰¾è¯„è®ºå®¹å™¨
                    const commentContainers = document.querySelectorAll('[class*="comment"], [class*="reply"], [class*="discussion"]');
                    
                    commentContainers.forEach(container => {
                        // æå–è¯„è®ºä½œè€…
                        const authorElement = container.querySelector('[class*="author"], [class*="user"], [class*="name"]');
                        const author = authorElement ? authorElement.textContent.trim() : 'åŒ¿åç”¨æˆ·';
                        
                        // æå–è¯„è®ºå†…å®¹
                        const contentElement = container.querySelector('[class*="content"], [class*="text"], [class*="body"]');
                        const content = contentElement ? contentElement.textContent.trim() : container.textContent.trim();
                        
                        // æå–è¯„è®ºæ—¶é—´
                        const timeElement = container.querySelector('[class*="time"], [class*="date"], time');
                        const time = timeElement ? timeElement.textContent.trim() : '';
                        
                        // æ£€æŸ¥æ˜¯å¦æ˜¯ä½œè€…å›å¤
                        const isAuthorReply = author.includes('é‡‘èå­¦é•¿') || author.includes('åœˆä¸»') || author.includes('æ¥¼ä¸»');
                        
                        if (content && content.length > 5) {
                            comments.push({
                                author: author,
                                content: content,
                                date: time,
                                is_author_reply: isAuthorReply,
                                type: 'comment'
                            });
                        }
                    });
                    
                    return comments;
                }
            """)
            
            return comments_data
            
        except Exception as e:
            logger.error(f"æå–è¯„è®ºå¤±è´¥: {e}")
            return []
    
    def _parse_comments_from_content(self, content: str) -> List[Dict]:
        """ä»å¸–å­å†…å®¹ä¸­è§£æè¯„è®º"""
        comments = []
        
        if not content:
            return comments
        
        lines = content.split('\n')
        current_comment = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹è¯„è®ºå¼€å§‹
            if self._is_comment_start(line):
                if current_comment and current_comment.get('content'):
                    comments.append(current_comment)
                
                current_comment = {
                    'author': self._extract_comment_author(line),
                    'content': '',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'is_author_reply': self._is_author_reply(line),
                    'type': 'comment'
                }
                continue
            
            # æ”¶é›†è¯„è®ºå†…å®¹
            if current_comment:
                if not self._is_metadata_line(line) and len(line) > 5:
                    current_comment['content'] += line + '\n'
        
        # æ·»åŠ æœ€åä¸€ä¸ªè¯„è®º
        if current_comment and current_comment.get('content'):
            comments.append(current_comment)
        
        return comments
    
    def _is_comment_start(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºè¯„è®ºå¼€å§‹"""
        comment_keywords = ['å›å¤', 'è¯„è®º', 'è¯´:', ':', 'ï¼š']
        return any(keyword in line for keyword in comment_keywords) and len(line) > 5
    
    def _extract_comment_author(self, line: str) -> str:
        """æå–è¯„è®ºä½œè€…"""
        # ç®€å•çš„ä½œè€…æå–é€»è¾‘
        if ':' in line:
            return line.split(':')[0].strip()
        elif 'ï¼š' in line:
            return line.split('ï¼š')[0].strip()
        else:
            return 'åŒ¿åç”¨æˆ·'
    
    def _is_author_reply(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯ä½œè€…å›å¤"""
        author_keywords = ['é‡‘èå­¦é•¿', 'åœˆä¸»', 'æ¥¼ä¸»', 'ä½œè€…']
        return any(keyword in line for keyword in author_keywords)
    
    async def _go_to_next_page(self, page) -> bool:
        """è·³è½¬åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_selectors = [
                '.pagination .next',
                '.pagination-next',
                '.next-page',
                'a[rel="next"]',
                'button:has-text("ä¸‹ä¸€é¡µ")',
                'a:has-text("ä¸‹ä¸€é¡µ")'
            ]
            
            for selector in next_selectors:
                next_button = await page.query_selector(selector)
                if next_button:
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                    is_disabled = await next_button.get_attribute('disabled')
                    if not is_disabled:
                        await next_button.click()
                        await page.wait_for_timeout(3000)
                        logger.info("å·²è·³è½¬åˆ°ä¸‹ä¸€é¡µ")
                        return True
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå°è¯•æ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘åŠ è½½
            logger.info("æœªæ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå°è¯•æ»šåŠ¨è§¦å‘åŠ è½½")
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            current_content = await self._get_page_content(page)
            if len(current_content) > 1000:  # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å®¹
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"è·³è½¬ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            return False
    
    async def _scroll_page(self, page, scroll_count: int = 3):
        """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹"""
        for i in range(scroll_count):
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self._random_delay(1, 3)
            logger.info(f"æ»šåŠ¨é¡µé¢ {i+1}/{scroll_count}")
    
    async def _get_page_content(self, page):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            # ç­‰å¾…é¡µé¢ç¨³å®š
            await page.wait_for_load_state('networkidle')
            await page.wait_for_timeout(2000)
            
            # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
            content = await page.evaluate("""
                () => {
                    try {
                        // è·å–é¡µé¢æ ‡é¢˜
                        const title = document.title || '';
                        
                        // è·å–ä¸»è¦æ–‡æœ¬å†…å®¹
                        const mainContent = document.querySelector('.main-content, .content, .post-content, .article-content') || document.body;
                        
                        // è·å–æ‰€æœ‰å¯è§æ–‡æœ¬
                        const walker = document.createTreeWalker(
                            mainContent,
                            NodeFilter.SHOW_TEXT,
                            null,
                            false
                        );
                        
                        let textNodes = [];
                        let node;
                        while (node = walker.nextNode()) {
                            if (node.parentElement && 
                                node.parentElement.offsetParent !== null && 
                                node.textContent.trim()) {
                                textNodes.push(node.textContent.trim());
                            }
                        }
                        
                        return textNodes.join('\\n');
                    } catch (e) {
                        return 'è·å–å†…å®¹å¤±è´¥: ' + e.message;
                    }
                }
            """)
            
            return content
            
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            return ""
    
    async def _random_delay(self, min_seconds: float, max_seconds: float):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        delay = random.uniform(min_seconds, max_seconds)
        await asyncio.sleep(delay)
    
    def _parse_essence_posts(self, content: str) -> List[Dict]:
        """è§£æç²¾åå¸–å­å†…å®¹"""
        posts = []
        
        if not content:
            return posts
        
        lines = content.split('\n')
        current_post = {}
        collecting_content = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹ç²¾åå¸–å­å¼€å§‹
            if self._is_post_start(line):
                # ä¿å­˜ä¸Šä¸€ä¸ªå¸–å­
                if current_post and current_post.get('content'):
                    posts.append(current_post)
                
                # å¼€å§‹æ–°å¸–å­
                current_post = {
                    'title': self._extract_title(line),
                    'content': '',
                    'author': 'é‡‘èå­¦é•¿',
                    'publish_date': self._extract_date(line),
                    'likes_count': 0,
                    'comments_count': 0,
                    'view_count': 0,
                    'is_pinned': False,
                    'tags': '',
                    'summary': '',
                    'page_number': 1
                }
                collecting_content = True
                continue
            
            # æ”¶é›†å¸–å­å†…å®¹
            if collecting_content and current_post:
                if not self._is_metadata_line(line):
                    if len(line) > 10:
                        current_post['content'] += line + '\n'
        
        # æ·»åŠ æœ€åä¸€ä¸ªå¸–å­
        if current_post and current_post.get('content'):
            posts.append(current_post)
        
        return posts
    
    def _is_post_start(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºå¸–å­å¼€å§‹"""
        essence_keywords = ['ç²¾å', 'ã€ç²¾åã€‘', '[ç²¾å]', 'ç½®é¡¶ç²¾å']
        return any(keyword in line for keyword in essence_keywords) and len(line) > 5
    
    def _extract_title(self, line: str) -> str:
        """æå–æ ‡é¢˜"""
        title = line.replace('ã€ç²¾åã€‘', '').replace('[ç²¾å]', '').replace('ç²¾å', '').strip()
        return title if title else "ç²¾åå¸–å­"
    
    def _extract_date(self, line: str) -> str:
        """æå–æ—¥æœŸ"""
        today = datetime.now()
        return today.strftime('%Y-%m-%d')
    
    def _is_metadata_line(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºå…ƒæ•°æ®è¡Œ"""
        metadata_patterns = [
            'æŸ¥çœ‹å…¨æ–‡', 'èµ', 'è¯„è®º', 'å›å¤', 'æŸ¥çœ‹æ‰€æœ‰è¯„è®º',
            'è¡¨æƒ…', 'å›¾ç‰‡', 'æ–‡ä»¶', 'éŸ³é¢‘', 'å†™æ–‡ç« ', 'å‘å¸ƒ'
        ]
        return any(pattern in line for pattern in metadata_patterns)
    
    def _filter_recent_half_year(self, posts: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤è¿‘åŠå¹´çš„å¸–å­"""
        six_months_ago = datetime.now() - timedelta(days=180)
        recent_posts = []
        
        for post in posts:
            publish_date = post.get('publish_date', '')
            if not publish_date:
                continue
            
            try:
                if '-' in publish_date:
                    post_date = datetime.strptime(publish_date, '%Y-%m-%d')
                else:
                    continue
                
                if post_date >= six_months_ago:
                    recent_posts.append(post)
                    
            except ValueError:
                continue
        
        return recent_posts
    
    def _save_posts_with_comments(self, posts: List[Dict]):
        """ä¿å­˜å¸–å­åŠå…¶è¯„è®ºåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_posts = 0
        saved_comments = 0
        
        for post in posts:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ ‡é¢˜å’Œé¡µç çš„å¸–å­
            cursor.execute('''
                SELECT id FROM essence_posts 
                WHERE title = ? AND author = ? AND publish_date = ? AND page_number = ?
            ''', (
                post.get('title', ''), 
                post.get('author', ''), 
                post.get('publish_date', ''), 
                post.get('page_number', 1)
            ))
            
            existing = cursor.fetchone()
            
            if not existing:
                cursor.execute('''
                    INSERT INTO essence_posts (
                        title, content, author, publish_date, likes_count,
                        comments_count, view_count, is_pinned, tags, summary, page_number, post_url
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    post.get('title', ''),
                    post.get('content', ''),
                    post.get('author', ''),
                    post.get('publish_date', ''),
                    post.get('likes_count', 0),
                    post.get('comments_count', 0),
                    post.get('view_count', 0),
                    post.get('is_pinned', False),
                    post.get('tags', ''),
                    post.get('summary', ''),
                    post.get('page_number', 1),
                    post.get('post_url', '')
                ))
                post_id = cursor.lastrowid
                saved_posts += 1
                
                # ä¿å­˜è¯„è®º
                comments = post.get('comments', [])
                for comment in comments:
                    cursor.execute('''
                        INSERT INTO post_comments (
                            post_id, comment_author, comment_content, comment_date, 
                            is_author_reply, likes_count, comment_type
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        post_id,
                        comment.get('author', 'åŒ¿åç”¨æˆ·'),
                        comment.get('content', ''),
                        comment.get('date', datetime.now().strftime('%Y-%m-%d')),
                        comment.get('is_author_reply', False),
                        comment.get('likes_count', 0),
                        comment.get('type', 'comment')
                    ))
                    saved_comments += 1
        
        conn.commit()
        conn.close()
        logger.info(f"æˆåŠŸä¿å­˜ {saved_posts} ä¸ªç²¾åå¸–å­å’Œ {saved_comments} æ¡è¯„è®ºåˆ°æ•°æ®åº“")
        return saved_posts, saved_comments
    
    def _save_essence_posts(self, posts: List[Dict]):
        """ä¿å­˜ç²¾åå¸–å­åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        for post in posts:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ ‡é¢˜å’Œé¡µç çš„å¸–å­
            cursor.execute('''
                SELECT id FROM essence_posts 
                WHERE title = ? AND author = ? AND publish_date = ? AND page_number = ?
            ''', (
                post.get('title', ''), 
                post.get('author', ''), 
                post.get('publish_date', ''), 
                post.get('page_number', 1)
            ))
            
            existing = cursor.fetchone()
            
            if not existing:
                cursor.execute('''
                    INSERT INTO essence_posts (
                        title, content, author, publish_date, likes_count,
                        comments_count, view_count, is_pinned, tags, summary, page_number
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    post.get('title', ''),
                    post.get('content', ''),
                    post.get('author', ''),
                    post.get('publish_date', ''),
                    post.get('likes_count', 0),
                    post.get('comments_count', 0),
                    post.get('view_count', 0),
                    post.get('is_pinned', False),
                    post.get('tags', ''),
                    post.get('summary', ''),
                    post.get('page_number', 1)
                ))
                saved_count += 1
        
        conn.commit()
        conn.close()
        logger.info(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªç²¾åå¸–å­åˆ°æ•°æ®åº“")
    
    def _log_crawl_success(self, pages_crawled: int, posts_count: int):
        """è®°å½•æˆåŠŸçš„æŠ“å–æ—¥å¿—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO crawl_logs (crawl_date, pages_crawled, posts_count, success, error_message)
            VALUES (?, ?, ?, ?, ?)
        ''', (datetime.now().strftime('%Y-%m-%d'), pages_crawled, posts_count, True, ''))
        
        conn.commit()
        conn.close()
    
    def _log_crawl_success_with_comments(self, pages_crawled: int, posts_count: int, comments_count: int):
        """è®°å½•æˆåŠŸçš„æŠ“å–æ—¥å¿—ï¼ˆåŒ…å«è¯„è®ºç»Ÿè®¡ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO crawl_logs (crawl_date, pages_crawled, posts_count, comments_count, success, error_message)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (datetime.now().strftime('%Y-%m-%d'), pages_crawled, posts_count, comments_count, True, ''))
        
        conn.commit()
        conn.close()
    
    def _log_crawl_failure(self, error_message: str):
        """è®°å½•å¤±è´¥çš„æŠ“å–æ—¥å¿—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO crawl_logs (crawl_date, pages_crawled, posts_count, success, error_message)
            VALUES (?, ?, ?, ?, ?)
        ''', (datetime.now().strftime('%Y-%m-%d'), 0, 0, False, error_message))
        
        conn.commit()
        conn.close()
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM essence_posts')
        total_posts = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT page_number) FROM essence_posts')
        total_pages = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM crawl_logs WHERE success = 1')
        successful_crawls = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM crawl_logs WHERE success = 0')
        failed_crawls = cursor.fetchone()[0]
        
        # è·å–æœ€è¿‘çš„æŠ“å–è®°å½•
        cursor.execute('SELECT crawl_date, pages_crawled, posts_count FROM crawl_logs ORDER BY id DESC LIMIT 1')
        latest_crawl = cursor.fetchone()
        
        conn.close()
        
        stats = {
            'total_posts': total_posts,
            'total_pages': total_pages,
            'successful_crawls': successful_crawls,
            'failed_crawls': failed_crawls,
            'latest_crawl': latest_crawl
        }
        
        return stats


async def main():
    """ä¸»å‡½æ•°"""
    crawler = EnhancedEssenceCrawler()
    
    print("å¢å¼ºç‰ˆå°çº¢åœˆç²¾åå¸–å­æŠ“å–ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
    stats = crawler.get_database_stats()
    print(f"å½“å‰æ•°æ®åº“ç»Ÿè®¡:")
    print(f"æ€»ç²¾åå¸–å­æ•°: {stats['total_posts']}")
    print(f"æ€»æŠ“å–é¡µæ•°: {stats['total_pages']}")
    print(f"æˆåŠŸæŠ“å–æ¬¡æ•°: {stats['successful_crawls']}")
    print(f"å¤±è´¥æŠ“å–æ¬¡æ•°: {stats['failed_crawls']}")
    
    if stats['latest_crawl']:
        date, pages, count = stats['latest_crawl']
        print(f"æœ€è¿‘æŠ“å–: {date} (æŠ“å– {pages} é¡µ, {count} ä¸ªå¸–å­)")
    
    print("\nè¯·é€‰æ‹©æŠ“å–æ¨¡å¼:")
    print("1. ä»…æŠ“å–ç²¾åå¸–å­ï¼ˆå¿«é€Ÿï¼‰")
    print("2. æŠ“å–ç²¾åå¸–å­åŠè¯„è®ºå›å¤ï¼ˆå®Œæ•´ï¼‰")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-2): ").strip()
    
    if choice == "1":
        print("\nå¼€å§‹åˆ†é¡µæŠ“å–ç²¾åå¸–å­...")
        print("æ³¨æ„: ç³»ç»Ÿå°†è‡ªåŠ¨å¤„ç†ç™»å½•çŠ¶æ€å’Œåˆ†é¡µåŠ è½½")
        
        # æ‰§è¡ŒæŠ“å–
        success = await crawler.crawl_essence_posts_with_pagination(max_pages=10)
        
        if success:
            print("âœ… ç²¾åå¸–å­åˆ†é¡µæŠ“å–å®Œæˆï¼")
            
            # æ˜¾ç¤ºæ›´æ–°åçš„ç»Ÿè®¡
            new_stats = crawler.get_database_stats()
            print(f"\næ›´æ–°åç»Ÿè®¡:")
            print(f"æ€»ç²¾åå¸–å­æ•°: {new_stats['total_posts']}")
            print(f"æ€»æŠ“å–é¡µæ•°: {new_stats['total_pages']}")
            
            if new_stats['latest_crawl']:
                date, pages, count = new_stats['latest_crawl']
                print(f"æœ¬æ¬¡æŠ“å–: {date} (æŠ“å– {pages} é¡µ, {count} ä¸ªå¸–å­)")
            
            # æ˜¾ç¤ºæ•°æ®åº“æ–‡ä»¶ä¿¡æ¯
            db_size = os.path.getsize(crawler.db_path) if os.path.exists(crawler.db_path) else 0
            print(f"æ•°æ®åº“æ–‡ä»¶: {crawler.db_path}")
            print(f"æ•°æ®åº“å¤§å°: {db_size / 1024:.2f} KB")
            
            print("\nğŸ‰ å¢å¼ºç‰ˆç³»ç»ŸåŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
            
        else:
            print("âŒ ç²¾åå¸–å­æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç½‘ç«™çŠ¶æ€")
    
    elif choice == "2":
        print("\nå¼€å§‹åˆ†é¡µæŠ“å–ç²¾åå¸–å­åŠè¯„è®º...")
        print("æ³¨æ„: ç³»ç»Ÿå°†è‡ªåŠ¨å¤„ç†ç™»å½•çŠ¶æ€å’Œåˆ†é¡µåŠ è½½")
        print("âš ï¸  è¯„è®ºæŠ“å–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
        
        # æ‰§è¡ŒæŠ“å–
        success = await crawler.crawl_essence_posts_with_comments(max_pages=5)  # å…ˆæµ‹è¯•5é¡µ
        
        if success:
            print("âœ… ç²¾åå¸–å­åŠè¯„è®ºåˆ†é¡µæŠ“å–å®Œæˆï¼")
            
            # æ˜¾ç¤ºæ›´æ–°åçš„ç»Ÿè®¡
            new_stats = crawler.get_database_stats()
            print(f"\næ›´æ–°åç»Ÿè®¡:")
            print(f"æ€»ç²¾åå¸–å­æ•°: {new_stats['total_posts']}")
            print(f"æ€»æŠ“å–é¡µæ•°: {new_stats['total_pages']}")
            
            # è·å–è¯„è®ºç»Ÿè®¡
            conn = sqlite3.connect(crawler.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM post_comments')
            total_comments = cursor.fetchone()[0]
            conn.close()
            
            print(f"æ€»è¯„è®ºæ•°: {total_comments}")
            
            if new_stats['latest_crawl']:
                date, pages, count = new_stats['latest_crawl']
                print(f"æœ¬æ¬¡æŠ“å–: {date} (æŠ“å– {pages} é¡µ, {count} ä¸ªå¸–å­)")
            
            # æ˜¾ç¤ºæ•°æ®åº“æ–‡ä»¶ä¿¡æ¯
            db_size = os.path.getsize(crawler.db_path) if os.path.exists(crawler.db_path) else 0
            print(f"æ•°æ®åº“æ–‡ä»¶: {crawler.db_path}")
            print(f"æ•°æ®åº“å¤§å°: {db_size / 1024:.2f} KB")
            
            print("\nğŸ‰ å¢å¼ºç‰ˆç³»ç»ŸåŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
            
        else:
            print("âŒ ç²¾åå¸–å­åŠè¯„è®ºæŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç½‘ç«™çŠ¶æ€")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œç¨‹åºé€€å‡º")


if __name__ == "__main__":
    asyncio.run(main())
